********************************
PySpark Programming for Beginner
********************************
We will only briefly introduce how to program in PySpark to manipulate datasets
and achieve certain big data processing tasks.

PySpark Basics
==============
+ Concepts

  .. glossary::

    SparkContext
      Entry point to most of the PySpark functionalities.

    RDD (Resilient Distributed Dataset)
      A fundamental building block of PySpark which is fault-tolerant, immutable distributed collection of objects

    DataFrame
      A distributed collection of data organized into named columns.

    DataSet
      Similar to DataFrame but with strict types.

Related Python Concepts
=======================
We will need to learn some useful python snippets to be used in data
manipulation using PySpark APIs.

Lambda Functions
----------------
+ A simple and disposable way to write a python function.
+ Many RDD transformations require a function as its parameter
+ Syntax

  * ``lambda <input list>: <return value>``
  * start with ``lambda`` keyword
  * followed by a list of parameters
  * after the colon, provide the return value calculated based on the
    parameters

+ Examples

  * ``lambda x: x*x``, square the input
  * ``lambda line: line.split()``, split a text to a list of text using spaces
    as delimiters
  * ``lambda paragraph: paragraph.split('\n')``, split a text to a list of
    lines using new line character as delimiters
  * ``lambda word: (word, 1)``, convert a work to a key-value pair of word and
    1
  * ``lambda pair: pair[0]``, return the first element of the pair
  * ``lambda pair: pair[1]``, return the second element of the pair
  * ``lambda x, y: x+y``, sum up two value
  * ``lambda p1, p2: p1[1] + p2[1]``, sum up the second values in two pairs

RDD (Resilient Distributed Dataset)
===================================
+ External readings

  * `RDD Programming Guide <rdd1_>`_
  * `A Comprehensive Guide to PySpark RDD Operations <av1_>`_

.. _rdd1: https://spark.apache.org/docs/latest/rdd-programming-guide.html
.. _av1: https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-to-pyspark-rdd-operations/

+ Two types of operations

  * Transformations

    - Takes an RDD and return another RDD
    - Distributed to distributed
    - Include but more than just mapping

  * Actions

    - Take an RDD and return a single object
    - Distributed to local
    - Include but more than just reducing

+ Useful transformations

  * ``map``
  * ``flatmap``
  * ``filter``

+ Useful actions

  * ``collect``
  * ``count``
  * ``first``
  * ``take``
  * ``reduce``
  * ``sum``
  * ``max``
  * ``min``
  * ``mean``
  * ``countByValue``

+ Pair RDD

  PySpark provides a specialized group of operations for working with Pair
  RDDs. In PySpark, pair RDDs are a specialized subtype of the RDD data
  structure that take the form of key-value pairs.

+ Useful transformations for pair RDDs

  * ``reduceByKey``
  * ``sortByKey``
  * ``groupByKey``

+ Useful actions for pair RDDs

  * ``countByKey``
