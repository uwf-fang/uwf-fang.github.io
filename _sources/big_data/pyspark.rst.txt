********************************
PySpark Programming for Big Data
********************************
Spark support multiple programming languages including Java, Scala, Python, and
R. In this course, we will use Python as the programming language to work with
Spark. The PySpark components are built on top of the Spark core and provide
Python APIs to interact with Spark.

We will only briefly introduce how to program in PySpark to manipulate datasets
and achieve certain big data processing tasks.

Why PySpark
===========
+ Competitors

  * Scala

    - native language for Spark
    - best performance
    - concise and powerful language
    - the language itself is not worth learning as a new language compared to
      Python
    - complicated syntax

  * Java: too verbose
  * R: not general-purpose

+ Advantages

  * Easier to install using pip than Scala
  * You may have learned Python already
  * Python worth the time to learn

PySpark Basics
==============
+ Concepts

  .. glossary::

    SparkContext
      Entry point to most of the PySpark functionalities.

    RDD (Resilient Distributed Dataset)
      A fundamental building block of PySpark which is fault-tolerant, immutable distributed collection of objects

    DataFrame
      A distributed collection of data organized into named columns. Optional
      in this course.

    DataSet
      Similar to DataFrame but with strict types. Optional in this course.

Why RDD only here?
------------------
The DataFrame and DataSet APIs are optional as it is not a typical way in big
data processing where data are mostly not structured. They are both closer to
either the SQL or the Pandas way of data manipulation and can be learned easily
if you are familiar with them.

On the other hand, RDD is the fundamental data API that provides more
capability to work with unstructured/semi-structured data. It the unique way to
work with big data in PySpark and is the focus here.

Related Python Concepts
=======================
As the programming language used in PySpark is Python, we will need to know
some Python concepts to work with PySpark. Please refer to the Python
programming resources online if you are not familiar with Python.

We will learn some useful python concepts to be used in data manipulation using
PySpark APIs.

Lambda Functions
----------------
+ A simple and disposable way to write a python function.
+ Many RDD transformations require a function as its parameter
+ Syntax

  * ``lambda <input list>: <return value>``
  * start with ``lambda`` keyword
  * followed by a list of parameters
  * after the colon, provide the return value calculated based on the
    parameters

+ Examples

  * ``lambda x: x*x``, square the input
  * ``lambda line: line.split()``, split a text to a list of text using spaces
    as delimiters
  * ``lambda paragraph: paragraph.split('\n')``, split a text to a list of
    lines using new line character as delimiters
  * ``lambda word: (word, 1)``, convert a work to a key-value pair of word and
    1
  * ``lambda word: len(word)``, convert a string to its length
  * ``lambda pair: pair[0]``, return the first element of the pair
  * ``lambda pair: pair[1]``, return the second element of the pair
  * ``lambda x, y: x+y``, sum up two value
  * ``lambda p1, p2: p1[1] + p2[1]``, sum up the second values in two pairs

RDD (Resilient Distributed Dataset)
===================================
+ External readings

  * `RDD Programming Guide <rdd1_>`_
  * `A Comprehensive Guide to PySpark RDD Operations <av1_>`_

.. _rdd1: https://spark.apache.org/docs/latest/rdd-programming-guide.html
.. _av1: https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-to-pyspark-rdd-operations/

+ Characteristics

  * Distributed (data is distributed across multiple machines)
  * Immutable (once created, it cannot be changed)
  * Lazy evaluation (transformations are not executed until an action is
    called)

+ Two types of operations

  * As extensions to traditional map/reduce paradigm
  * Transformations

    - Takes an RDD and return another RDD
    - Distributed RDD to distributed RDD
    - Can be chained
    - Include but more than just mapping

  * Actions

    - Take an RDD and return a single object
    - Distributed RDD to local object
    - Include but more than just reducing

+ Useful transformations

  * ``map(func)``, func should transform one item to another
  * ``flatMap(func)``, func should take one item and generate a list
  * ``filter(func)``, func should return True or False according to the input

+ Useful actions

  * ``collect()``, collect all data and return a local list, be careful with
    large dataset
  * ``count()``
  * ``first()``
  * ``take(num)``, take the first num of values and return a local list
  * ``reduce(func)``, func should take two parameters and give one result
  * ``sum()``
  * ``max()``
  * ``min()``
  * ``mean()``
  * ``countByValue()``

    - Python dict like type as output ``{key1: value1, key2: value2, ...}``
    - models key-value pair
    - key must be unique

+ Pair RDD

  PySpark provides a specialized group of operations for working with Pair
  RDDs. In PySpark, pair RDDs are a specialized subtype of the RDD data
  structure that take the form of key-value pairs.

  * example ``[(key1, value1), (key2, value2), ...]``
  * Useful transformations for pair RDDs

    - ``sortByKey(ascending=True)``
    - ``sortBy(func, ascending=True)``

      * func should take an item and return the value used to perform sorting
      * e.g. ``pairs.sortBy(lambda p: p[1])`` use the value (second item in the
        pair) to sort the pairs

    - ``groupByKey()``

      * create a new RDD with each item being pairs with a same key

  * Useful actions for pair RDDs

    - ``reduceByKey(func)``

      * func should take two values (second item in the pair) and return one
      * used to combine values that have a same key

    - ``countByKey()``

