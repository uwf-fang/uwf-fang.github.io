********************************
PySpark Programming for Beginner
********************************
We will only briefly introduce how to program in PySpark to manipulate datasets
and achieve certain big data processing tasks.

PySpark Basics
==============
+ Concepts

  .. glossary::

    SparkContext
      Entry point to most of the PySpark functionalities.

    RDD (Resilient Distributed Dataset)
      A fundamental building block of PySpark which is fault-tolerant, immutable distributed collection of objects

    DataFrame
      A distributed collection of data organized into named columns.

    DataSet
      Similar to DataFrame but with strict types.

Related Python Concepts
=======================
We will need to learn some useful python snippets to be used in data
manipulation using PySpark APIs.

Lambda Functions
----------------
+ A simple and disposable way to write a python function.
+ Many RDD transformations require a function as its parameter
+ Syntax

  * ``lambda <input list>: <return value>``
  * start with ``lambda`` keyword
  * followed by a list of parameters
  * after the colon, provide the return value calculated based on the
    parameters

+ Examples

  * ``lambda x: x*x``, square the input
  * ``lambda line: line.split()``, split a text to a list of text using spaces
    as delimiters
  * ``lambda paragraph: paragraph.split('\n')``, split a text to a list of
    lines using new line character as delimiters
  * ``lambda word: (word, 1)``, convert a work to a key-value pair of word and
    1
  * ``lambda word: len(word)``, convert a string to its length
  * ``lambda pair: pair[0]``, return the first element of the pair
  * ``lambda pair: pair[1]``, return the second element of the pair
  * ``lambda x, y: x+y``, sum up two value
  * ``lambda p1, p2: p1[1] + p2[1]``, sum up the second values in two pairs

RDD (Resilient Distributed Dataset)
===================================
+ External readings

  * `RDD Programming Guide <rdd1_>`_
  * `A Comprehensive Guide to PySpark RDD Operations <av1_>`_

.. _rdd1: https://spark.apache.org/docs/latest/rdd-programming-guide.html
.. _av1: https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-to-pyspark-rdd-operations/

+ Characteristics

  * Distributed
  * Immutable
  * Lazy evaluation

+ Two types of operations

  * Transformations

    - Takes an RDD and return another RDD
    - Distributed to distributed
    - Include but more than just mapping

  * Actions

    - Take an RDD and return a single object
    - Distributed to local
    - Include but more than just reducing

+ Useful transformations

  * ``map(func)``, func should transform one item to another
  * ``flatMap(func)``, func should take one item and generate a list
  * ``filter(func)``, func should return True or False according to the input

+ Useful actions

  * ``collect()``, collect all data and return a local list
  * ``count()``
  * ``first()``
  * ``take(num)``, take the first num of values and return a local list
  * ``reduce(func)``, func should take two parameters and give one result
  * ``sum()``
  * ``max()``
  * ``min()``
  * ``mean()``
  * ``countByValue()``

    - Python dict like type as output ``{key1: value1, key2: value2, ...}``
    - models key-value pair
    - key must be unique

+ Pair RDD

  PySpark provides a specialized group of operations for working with Pair
  RDDs. In PySpark, pair RDDs are a specialized subtype of the RDD data
  structure that take the form of key-value pairs.

  * example ``[(key1, value1), (key2, value2), ...]``
  * Useful transformations for pair RDDs

    - ``sortByKey(ascending=True)``
    - ``sortBy(func, ascending=True)``

      * func should take an item and return the value used to perform sorting
      * e.g. ``pairs.sortBy(lambda p: p[1]`` use the value (second item in the
        pair) to sort the pairs

    - ``groupByKey()``

      * create a new RDD with each item being pairs with a same key

  * Useful actions for pair RDDs

    - ``reduceByKey(func)``

      * func should take two values (second item in the pair) and return one
      * used to combine values that have a same key

    - ``countByKey()``

