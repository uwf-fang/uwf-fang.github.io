*******************
Complexity Analysis
*******************
Computational complexity analysis plays a pivotal role in the theoretical
analysis of algorithms. It involves assessing how the performance of an
algorithm scales with the size of the input, enabling us to make predictions
about an algorithm's efficiency in terms of time and space usage. This analysis
is crucial for understanding the inherent difficulty of solving computational
problems and for comparing and selecting algorithms for various tasks.

Concepts
========

.. glossary::

  Computational complexity
    The computational resources consumed by an algorithm as a function of the
    size of the problem.

  Time complexity
    The computational time consumed by an algorithm as a function of the size of
    the problem. It is usually described using a function :math:`T(n)`.

  Space complexity
    The computational storage space (usually refer to memory) consumed by an
    algorithm as a function of the size of the problem. **auxiliary space
    complexity** is the space complexity excluding the input data. It is usually
    described using a function :math:`S(n)`.

  Best case, worst case, and average case
    Best case is the scenario in which the algorithm finished with the least
    resource consumed. Worst case is the scenario in which the algorithm
    finished with the most resource consumed. Sometimes we will use average
    case, which is the average over all possible inputs.

  Constant time operation
    Because the accurate measurement of time consumption is impossible, we
    usually count the number of constant time operations to estimate the time
    consumption. A constant time operation is an operation that always consume a
    certain amount of time regardless of the problem size n.

  Growth rate
    How much more resource is needed when the problem size grows. It is usually
    described using simple math functions. It is a simple way to visualize the
    complexity.

  Asymptotic notation
    The math language we use to describe the complexity in a formal way. It
    includes :math:`O` (big-O), :math:`\Omega` (big-omega), :math:`\Theta`
    (big-theta) notations.

  Lower bound
    The best (least resource) complexity of a given algorithm. Related to the best case and the :math:`\Omega` (big-omega) notation.

  Upper bound
    The worst (most resource) complexity of a given algorithm. Related to the worst case and the :math:`O` (big-O) notation.

  Exact bound
    When a same complexity can be found for both the upper and lower bound, it
    is the exact bound :math:`\Theta`

.. image:: /_static/complexity.png
  :width: 400
  :alt: Complexity
  :align: center

Asymptotic notation
===================
A.k.a Bachmannâ€“Landau notation. A mathematical notation system to describe the
limiting behavior of a function.

.. note::

  Time complexity is more useful and is focused in the following sections. Most
  of the rules can be used with space complexity with little modification.

Formal Definitions
------------------
Given :math:`T(n)` the time function of the problem size :math:`n`

We will write :math:`T(n) = O(f(n))` or :math:`T(n) \in O(f(n))` if there
exists a certain positive constant :math:`c` and a problem size :math:`n_{0}`,
such that :math:`T(n) \leq cf(n)` for all :math:`n \geq n_{0}`.

We will write :math:`T(n) = \Omega(f(n))` or :math:`T(n) \in \Omega(f(n))` if
there exists a certain positive constant :math:`c` and a problem size
:math:`n_{0}`, such that :math:`T(n) \geq cf(n)` for all :math:`n \geq n_{0}`.

We will write :math:`T(n) = \Theta(f(n))` if :math:`T(n) = \Omega(f(n))` and
:math:`T(n) = O(f(n))` are both true. Note: the constants are different.

We will write :math:`T(n) = o(f(n))` or :math:`T(n) \in O(f(n))` if there
exists a certain positive constant :math:`c` and a problem size :math:`n_{0}`,
such that :math:`T(n) < cf(n)` for all :math:`n \geq n_{0}`.

.. warning::

  The :math:`=` used in equations like :math:`T(n) = O(f(n))` does not mean
  *equal*! It means *in* or *belong to*.

All notations :math:`O(f(n))`, :math:`\Omega(f(n))`, :math:`T(n) =
\Theta(f(n))` and :math:`T(n) = o(f(n))` define sets of functions that have the
same *order*. Thus, the set will be the same of the function :math:`f(n)` has
the same order. For example, :math:`O(n^2)=O(2*n^2)=O(n^2+3n)=\dots`

Representative Algorithms of Certain Complexity
-----------------------------------------------

.. list-table:: Typical Complexity and Representative Algorithms
   :widths: 20 30 50
   :header-rows: 1

   * - Name
     - Complexity (Big O)
     - Representative Algorithms
   * - Constant
     - :math:`O(1)`
     - Array indexing, Hash table lookups
   * - Logarithmic
     - :math:`O(log(n))`
     - Binary Search, Balanced Binary Search Trees (e.g., AVL, Red-Black)
   * - Linear
     - :math:`O(n)`
     - Linear Search, Most Simple Array/Linked List Operations
   * - Linearithmic
     - :math:`O(n log(n))`
     - Merge Sort, Heap Sort, Quicksort
   * - Quadratic
     - :math:`O(n^2)`
     - Bubble Sort, Selection Sort, Insertion Sort
   * - Polynomial (k > 2)
     - :math:`O(n^k)`
     - Matrix Multiplication (naive approach)
   * - Exponential
     - :math:`O(2^n)`
     - Recursive algorithms without memoization (e.g., naive Fibonacci)
   * - Factorial
     - :math:`O(n!)`
     - Traveling Salesman Problem (naive solution)

.. Improve this!

Analysis
========
Find :math:`T(n)`
-----------------
+ additive (sequential, branch)
+ multiplicative (loop, recursion)

  * complexity of number of iterations

    - how problem size is reduced in each iteration`
    - reduce by half each round: :math:`O(log(n))`
    - reduce by a constant each round: :math:`O(n)`

  * complexity in each iteration

Find :math:`O` given :math:`T(n)`
---------------------------------
Given :math:`T(n) = 20 * n^2 + 10 * n + 12`, find the term with highest order,
and remove all constant factors to give :math:`O(n^2)`

Pitfalls
========
+ Confusing :math:`T(n)` to :math:`O(f(n))`
+ Not all function calls are constant time operations (a.k.a. :math:`O(1)`)
