***********************************
Module 7: Other Topics in Big data
***********************************
Streaming for Big Data
======================
+ Definition and importance

  Streaming for Big Data is a method of processing and analyzing large and
  complex data sets in real-time or near real-time. It involves a continuous
  flow of data that can be analyzed, processed, and acted upon in real-time as
  it is generated. The importance of streaming for Big Data lies in the fact
  that traditional data processing methods are not efficient in dealing with
  large and continuously generated data sets.

+ Characteristics

  * Real-time: Streaming for Big Data allows organizations to process data in
    real-time, which means they can act on data insights as they occur. This is
    particularly useful for time-sensitive data, such as financial transactions
    or sensor data from IoT devices.
  * Continuous: Streaming for Big Data involves a continuous flow of data that
    can be processed and analyzed without interruption. This means that
    organizations can receive and process data in real-time, without having to
    wait for batch processing. The continuous processing enables organizations
    to quickly and effectively respond to changing conditions.
  * Heterogeneous: Data streams involves data that is heterogeneous, meaning it
    can come from different sources and in various formats. It can include
    structured data from traditional databases, as well as unstructured data
    from social media feeds, sensor data from IoT devices, and more. This
    requires organizations to use advanced analytics techniques to clean,
    normalize, and process the data to extract meaningful insights.
  * Imperfect: Data streams are often imperfect, meaning it can contain errors,
    inconsistencies, or missing values. This requires organizations to use
    advanced data quality techniques to identify and fix issues in real-time.
    By doing so, they can avoid making decisions based on inaccurate or
    incomplete data.
  * Volatile: In the context of streaming for big data, "volatile" refers to
    the rapid and unpredictable rate of change of data. This characteristic is
    particularly relevant for data that is continuously flowing in real-time
    from various sources, such as sensors, social media feeds, and other
    streaming data.

    Organizations must understand the volatility of streaming data to determine
    how long it remains relevant and actionable. For instance, sentiment
    analysis of social media data can be highly volatile, as opinions can
    quickly shift and change. Conversely, certain types of streaming data, such
    as weather trends, may have a lower volatility as they are more
    predictable.

    Understanding the volatility of streaming data is crucial for effective
    data management and analysis. Organizations need to implement real-time
    analytics tools and techniques that can rapidly process and analyze the
    data as it streams in. Failure to properly manage volatile data in
    real-time can lead to inaccurate insights, missed opportunities, and
    increased business risks.

+ Advantages

  * Real-time: Real-time processing is a critical advantage of data streaming
    technology in big data applications. Real-time processing allows
    organizations to analyze and process data as it is generated, providing
    immediate insights and enabling timely decision-making. With data streaming
    technology, data can be analyzed and acted upon within milliseconds or
    seconds, which is particularly important in industries such as finance,
    healthcare, and transportation where real-time decision-making is critical.

    One of the key benefits of real-time processing is the ability to detect
    and respond to events as they happen. For example, in the financial
    industry, real-time processing can be used to detect fraudulent activities
    as they occur, preventing financial losses and maintaining customer trust.
    In healthcare, real-time processing can be used to monitor patients' vital
    signs, alerting doctors immediately if there are any signs of trouble. In
    transportation, real-time processing can be used to track the location of
    vehicles, predict traffic patterns, and optimize routes in real-time,
    improving efficiency and reducing costs. Real-time processing also enables
    organizations to take advantage of opportunities as they arise. For
    example, real-time analysis of customer behavior can provide insights into
    customer preferences and enable organizations to personalize marketing
    messages in real-time, increasing the likelihood of conversion and
    improving customer satisfaction.

  * Scalability is a key advantage of data streaming technology in big data
    applications. Data streaming technology can handle large volumes of data
    and scale easily to meet increasing data processing demands. As data
    volumes grow, data streaming technology can dynamically allocate more
    resources to handle the increased workload, ensuring that processing
    remains efficient and effective.

    One of the key reasons why data streaming technology is so scalable is its
    ability to process data in parallel. Data is broken down into small chunks
    and processed concurrently, which allows for higher throughput and reduced
    processing times. This parallel processing also allows for horizontal
    scaling, where additional computing resources can be added to the
    processing pipeline to handle increasing data volumes.

  * Flexibility: Data streaming technology is highly flexible and can be used
    in a variety of applications. It can be used for real-time monitoring,
    predictive analytics, fraud detection, and more. Data streaming technology
    is not limited to any specific type of data, and it can process data in
    many formats, including structured, semi-structured, and unstructured data.
    This flexibility allows for organizations to use data streaming technology
    for a wide range of use cases, such as financial services, healthcare,
    retail, manufacturing, and more.

    For example, in financial services, data streaming technology can be used
    for real-time monitoring of transactions to detect fraudulent activities.
    In healthcare, data streaming technology can be used to monitor patients'
    vital signs in real-time, providing doctors with instant alerts if any
    vital signs fall outside of acceptable ranges. In retail, data streaming
    technology can be used for real-time inventory management, ensuring that
    products are always available on the shelves.

    Furthermore, data streaming technology is often used in conjunction with
    other big data technologies, such as batch processing and machine learning,
    to provide a comprehensive solution for data processing and analysis. This
    flexibility enables organizations to tailor their big data infrastructure
    to their specific needs and requirements, allowing them to make
    better-informed decisions and gain a competitive advantage.

  * Reduced storage and processing costs:

    Reduced storage and processing costs are an important advantage of data
    streaming technology in big data applications. Traditional batch processing
    methods require large amounts of storage and processing power, which can be
    expensive and time-consuming. Data streaming technology, on the other hand,
    can significantly reduce storage and processing costs by processing data in
    real-time and discarding data that is no longer needed.

    With data streaming technology, data is processed as it is generated and
    analyzed in real-time. This means that data does not need to be stored in
    large quantities before processing, which can significantly reduce storage
    costs. In addition, since data is processed in real-time, processing
    resources can be allocated more efficiently, reducing processing costs.

+ Drawbacks of streaming

  * Streaming should be only added to the pipeline when the needs justify the
    cost! For example, in any data application that can perform well with
    historical data (a couple of hours or days old, for example), streaming is
    not necessary. In applications where real-time data is required, for
    example, fraud detection, stock trading, etc., streaming is a must.
  * Data quality issues: Data streaming technology can lead to data quality
    issues, such as missing values, duplicate records, and inconsistent data.
    This is because data is processed in real-time, which means that there is
    no time to clean and normalize the data before processing. This can lead to
    inaccurate insights and decisions, which can have a negative impact on
    business operations.
  * No backup of historical data and intermediate data
  * More complexity: Data streaming technology is more complex than traditional
    batch processing methods, which can make it difficult for organizations to
    implement and maintain. Data streaming technology requires specialized
    skills and expertise, which can be difficult to find and expensive to
    acquire. In addition, data streaming technology often requires the use of
    multiple tools and technologies, which can increase the complexity of the
    data processing pipeline.
  * Higher costs: Data streaming technology can be more expensive than other
    data processing methods, such as batch processing. Data streaming
    technology requires specialized skills and expertise, which can be
    difficult to find and expensive to acquire. In addition, data streaming
    technology often requires the use of multiple tools and technologies, which
    can increase the cost of the data processing pipeline.

+ Frameworks and Services

  * Apache Kafka: Apache Kafka is a distributed streaming platform that is
    designed to handle large volumes of data in real-time. It can process and
    analyze data as it is generated and enables seamless integration with other
    big data tools such as Apache Hadoop and Apache Spark.
  * Apache Flink: Apache Flink is a powerful open-source data streaming
    framework that is designed to process data in real-time with
    high-throughput and low-latency. It offers robust support for event-time
    processing, state management, and fault-tolerance.
  * Apache Spark Streaming: Apache Spark Streaming is an extension of the
    popular Apache Spark framework that allows for the processing of real-time
    data streams. It offers high throughput and low latency processing of data
    and supports multiple data sources.
  * Amazon Kinesis: Amazon Kinesis is a fully managed real-time streaming data
    service offered by Amazon Web Services. It allows users to capture,
    process, and analyze data streams in real-time at scale. Kinesis supports
    multiple data sources and is highly scalable.
  * Google Cloud Dataflow: Google Cloud Dataflow is a fully managed data
    processing service that supports both batch and stream processing. It
    offers a simple and easy-to-use programming model and can handle large
    volumes of data in real-time.
  * Microsoft Azure Stream Analytics: Microsoft Azure Stream Analytics is a
    fully managed real-time analytics service that allows users to process and
    analyze data streams from various sources. It offers seamless integration
    with other Azure services and supports both real-time and batch processing.
  * IBM Streams: IBM Streams is a high-performance data streaming platform that
    enables real-time processing and analysis of data streams. It offers a
    powerful programming model and can handle large volumes of data in
    real-time.

Cloud Computing For Big Data
============================
+ Definition

  Cloud computing is a model for delivering on-demand access to shared
  computing resources (such as servers, storage, applications, and services)
  over the internet. Big data refers to large and complex datasets that cannot
  be processed or analyzed using traditional data processing methods. Combining
  cloud computing with big data technologies can help businesses to manage,
  store, process and analyze their large datasets.

+ Characteristics

  * On-demand self-service: With on-demand self-service, users can access cloud
    computing resources and services quickly and easily, without needing to go
    through IT personnel. For example, a business may use a cloud computing
    service like Amazon Web Services (AWS) to spin up a cluster of virtual
    machines to process a large dataset. With just a few clicks, the business
    can provision the resources it needs and start processing data right away.
  * Broad network access: Broad network access allows users to access cloud
    computing resources and services over the internet, enabling them to work
    from anywhere using any kind of devices. For example, a data scientist may
    need to access a large dataset stored in the cloud from their home office
    or while traveling. With cloud computing, they can access the data from
    anywhere with an internet connection.
  * Resource pooling: Resource pooling enables multiple users to share
    computing resources and services, making it easier to scale up or down as
    demand changes. For example, a business may use a cloud computing service
    like Microsoft Azure to store and process large amounts of data. With
    resource pooling, the business can easily scale up or down its computing
    resources as needed, without needing to buy or maintain its own hardware.
  * Rapid elasticity: Rapid elasticity enables users to quickly scale up or
    down their computing resources and services as needed. For example, a
    business may need to process a large amount of data for a short period of
    time, such as during a sales promotion. With cloud computing, the business
    can quickly provision the additional resources it needs to handle the
    workload, and then de-provision them when the promotion is over.
  * Measured service: Measured service provides users with visibility into
    their usage of computing resources and services, enabling them to monitor
    and control costs more effectively. For example, a business may use a cloud
    computing service like Google Cloud Platform to store and process large
    amounts of data. With measured service, the business can track its usage of
    computing resources and services, and then adjust its usage to optimize
    costs.

+ Advantages of cloud computing

  * Cost-effective: One of the key advantages of cloud computing is its
    cost-effectiveness. With cloud computing, businesses can avoid the high
    upfront costs associated with purchasing and maintaining their own hardware
    and software infrastructure. Instead, they can pay for computing resources
    and services on an as-needed basis, which can be more cost-effective over
    time. Additionally, cloud computing can help businesses to reduce their
    energy costs and carbon footprint by enabling them to use computing
    resources more efficiently. For example, a business may use a cloud-based
    file storage service like Dropbox or Google Drive instead of maintaining
    its own file servers, which can save money on hardware, energy, and
    maintenance costs.
  * Scalable: Cloud computing is highly scalable, which means that businesses
    can easily adjust their computing resources and services to meet changing
    demands. With cloud computing, businesses can quickly provision additional
    computing resources when they need them and then de-provision them when
    they no longer need them. This can be particularly useful for businesses
    that experience spikes in demand, such as during peak sales periods or when
    launching new products. For example, a business may use a cloud-based
    e-commerce platform like Shopify or Magento to quickly scale up its online
    store during a busy holiday season.
  * Flexible: Cloud computing is highly flexible, which means that businesses
    can choose the computing resources and services that best meet their needs.
    With cloud computing, businesses can choose from a wide range of computing
    resources and services, including storage, processing power, networking,
    and more. Additionally, businesses can easily switch between different
    cloud providers or services as their needs change. For example, a business
    may use a cloud-based email service like Gmail or Outlook instead of
    maintaining its own email server, which can provide greater flexibility and
    scalability.
  * Reliable: Cloud computing is highly reliable, which means that businesses
    can expect high levels of uptime and availability. With cloud computing,
    businesses can rely on cloud providers to manage and maintain their
    computing resources and services, which can help to ensure that they are
    always available when needed. Additionally, cloud providers often offer
    backup and disaster recovery services, which can help businesses to recover
    quickly in the event of a data loss or outage. For example, a business may
    use a cloud-based backup service like Backblaze or Carbonite to ensure that
    its data is always backed up and protected.
  * Secure: Cloud computing is highly secure, which means that businesses can
    trust that their data and computing resources are protected from cyber
    threats. With cloud computing, businesses can rely on cloud providers to
    implement security measures like encryption, firewalls, and access controls
    to protect their data and resources. Additionally, cloud providers often
    have highly trained security teams that can quickly respond to security
    threats or incidents. For example, a business may use a cloud-based
    security service like Cloudflare or Akamai to protect its website or online
    applications from cyber attacks.

+ Cloud computing service models

  * Infrastructure as a Service (IaaS): IaaS is a cloud computing model that
    provides businesses with virtualized computing resources, such as virtual
    machines, storage, and networking. With IaaS, businesses can easily
    provision and manage their own computing infrastructure, without the need
    to purchase and maintain physical hardware. Examples of IaaS providers
    include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud
    Platform (GCP).
  * Platform as a Service (PaaS): PaaS is a cloud computing model that provides
    businesses with a platform for developing, deploying, and managing their
    own applications. With PaaS, businesses can focus on building and
    delivering applications, without the need to manage the underlying
    infrastructure. PaaS providers typically provide pre-built tools and
    services for application development, such as development frameworks,
    databases, and testing tools. Examples of PaaS providers include Heroku,
    Google App Engine, and Microsoft Azure App Service.
  * Software as a Service (SaaS): SaaS is a cloud computing model that provides
    businesses with access to pre-built software applications that are hosted
    in the cloud. With SaaS, businesses can easily access and use software
    applications without the need to install or manage the software on their
    own devices or servers. SaaS applications can include a wide range of
    software, such as customer relationship management (CRM) tools, project
    management software, and email services. Examples of SaaS providers include
    Salesforce, Dropbox, and Microsoft Office 365.
  * Function as a Service (FaaS): FaaS is a cloud computing model that provides
    businesses with serverless computing resources, which are used to run
    individual functions or pieces of code. With FaaS, businesses can focus on
    building and deploying specific functions or services, without the need to
    manage the underlying infrastructure. FaaS providers typically charge
    businesses based on the number of times their functions are executed or the
    amount of time they are executed for. Examples of FaaS providers include
    AWS Lambda, Google Cloud Functions, and Microsoft Azure Functions.

.. note::

  Extended reading:

  + `A General Introduction to Cloud Computing <https://www.digitalocean.com/community/tutorials/a-general-introduction-to-cloud-computing>`_

Security and Privacy
====================
+ Threats and risks in Big Data

  Big Data is a term used to describe the massive amounts of data generated by
  individuals, organizations, and machines. Big data poses several security and
  privacy risks that organizations need to consider, such as:

  * Unauthorized access: The sheer volume of data in big data environments
    makes it difficult to secure every single data point, making them more
    vulnerable to hacking and unauthorized access.
  * Malicious attacks: Big data can be a prime target for malicious actors
    looking to steal sensitive data, compromise systems, or cause harm to an
    organization.
  * Insider threats: Employees or contractors with access to big data can
    intentionally or unintentionally disclose confidential data, compromising
    security and privacy.
  * Compliance violations: If an organization fails to comply with relevant
    laws and regulations, it may face legal action, penalties, and loss of
    reputation.

+ Data security and privacy techniques

  * Authentication and access control

    Authentication and access control are critical for securing cloud computing
    environments. They involve the use of passwords, multi-factor
    authentication, biometric authentication, and access control lists to limit
    access to sensitive data.

  * Encryption and data masking

    Encryption is a technique used to protect sensitive data by converting it
    into a coded language that can only be accessed with a decryption key. Data
    masking is another technique used to protect sensitive data by replacing
    the original data with fake or obfuscated data.

  * Data backup and disaster recovery

    Data backup and disaster recovery are essential components of any cloud
    computing environment. These measures help ensure that data can be
    recovered in the event of a system failure, natural disaster, or cyber
    attack.

+ Laws and regulations

  Several laws and regulations govern data security and privacy in cloud
  computing environments. Some of the most significant ones include:

  * General Data Protection Regulation (GDPR)

    The GDPR is a European Union (EU) regulation that came into effect in 2018.
    It governs the processing, storage, and protection of personal data in the
    EU and aims to provide EU citizens with greater control over their personal
    data.

  * Related US federal laws and acts

    US do not have a single federal law that governs data security and privacy.
    Instead, there are several federal laws and acts that govern data security
    and privacy in the US, including:

    - Health Insurance Portability and Accountability Act (HIPAA)

      HIPAA is a federal law in the United States that was enacted in 1996. It
      sets standards for the privacy and security of health information,
      including electronic health records. HIPAA applies to healthcare
      providers, insurance companies, and other entities that handle protected
      health information. The law requires these entities to safeguard
      patients' personal and health information and to notify them if there is
      a breach of their information.

    - Family Educational Rights and Privacy Act (FERPA)

      FERPA is a federal law in the United States that protects the privacy of
      student education records. FERPA applies to schools that receive federal
      funding, including K-12 schools and colleges and universities. The law
      gives parents and students the right to access and review their education
      records, and it requires schools to obtain written consent before
      disclosing any personally identifiable information in these records.

    - The Fair Credit Reporting Act (FCRA)

      The FCRA is is a federal law in the United States that regulates the
      collection, dissemination, and use of consumer credit information. FCRA
      applies to consumer reporting agencies, lenders, and other entities that
      use credit information. The law requires these entities to ensure the
      accuracy and privacy of consumer credit information and to obtain written
      consent before accessing or sharing this information.

    - Federal Tort Claims Act (FTCA)

      The FTCA is a federal law that governs the processing, storage, and
      protection of personal information in the US. It came into effect in 1946
      and aims to protect the privacy of individuals' personal information.

  * State laws and acts

    - California Consumer Privacy Act (CCPA)

      The CCPA is a state-level law in California that governs the collection,
      storage, and use of personal data. It came into effect in 2020 and
      provides California residents with the right to know what data is being
      collected about them, the right to delete their data, and the right to
      opt-out of data sharing.
