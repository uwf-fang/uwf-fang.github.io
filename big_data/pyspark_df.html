<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>PySpark Programming: Spark SQL and DataFrame API &#8212; IanFang Course Note Hosting  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=fa3f9a06" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="canonical" href="https://uwf-fang.github.io/big_data/pyspark_df.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">IanFang Course Note Hosting  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">PySpark Programming: Spark SQL and DataFrame API</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="pyspark-programming-spark-sql-and-dataframe-api">
<h1>PySpark Programming: Spark SQL and DataFrame API<a class="headerlink" href="#pyspark-programming-spark-sql-and-dataframe-api" title="Link to this heading">¶</a></h1>
<section id="outline">
<h2>Outline<a class="headerlink" href="#outline" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Spark SQL Overview</p></li>
<li><p>Performance optimizations</p></li>
<li><p>Applications</p></li>
<li><p>DataFrame API</p></li>
</ul>
</section>
<section id="spark-sql-overview">
<h2>Spark SQL Overview<a class="headerlink" href="#spark-sql-overview" title="Link to this heading">¶</a></h2>
<p>Spark SQL is a Spark module for structured data processing. It is one of the
five Spark components: <strong>Spark Core, Spark SQL, Spark Streaming, MLlib,
GraphX</strong></p>
<p>Spark SQL provides a unified data access layer for various data sources. It
works on top of Spark Core and provides a higher-level abstractions to work
with structured data. Typical structured datasets are like tables in a
relational database or a sheet in Excel.</p>
<p>You can code in PySpark using the API calls or SQL queries depending on your
preference. The DataFrame and DataSet APIs provides rich functionalities to
programmers to work with structured data through object-oriented programming
paradigm. The SQL queries are more friendly to data analysts who are familiar
with SQL, who can create SQL queries as strings and execute them using Spark
SQL.</p>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">Example of DataFrame API</span><a class="headerlink" href="#id1" title="Link to this code">¶</a></div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># df1 and df2 are DataFrames</span>
<span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;key&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;address&#39;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">Example of SQL query</span><a class="headerlink" href="#id2" title="Link to this code">¶</a></div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df1</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;table1&quot;</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;table2&quot;</span><span class="p">)</span>

<span class="c1"># spark is an instance of SparkSession class</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">  SELECT table1.name, table2.address</span>
<span class="s2">  FROM table1</span>
<span class="s2">  JOIN table2 ON table1.key = table2.key</span>
<span class="s2">  WHERE table1.age &gt; 21</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="performance-optimizations">
<h2>Performance optimizations<a class="headerlink" href="#performance-optimizations" title="Link to this heading">¶</a></h2>
<p>Some are general optimizations, while some are conditional optimizations based
on the data source and the query.</p>
<ul class="simple">
<li><p>Reduced disk I/O</p></li>
<li><p>Partitioning (helps when data is partitioned and some partitions can be
skipped)</p></li>
<li><p>Columnar storage (helps when a few columns are queried)</p></li>
<li><p>In-memory columnar caching (helps columnar storage)</p></li>
<li><p>Skip-rows (helps when many rows are skipped)</p></li>
<li><p>Predicate pushdown (delegate certain operations to data source when data
source supports it)</p></li>
<li><p>Query optimization with Catalyst optimizer</p>
<ul>
<li><p>Analysis, logical optimization, physical planning, code generation stages</p></li>
</ul>
</li>
</ul>
</section>
<section id="applications-of-spark-sql">
<h2>Applications of Spark SQL<a class="headerlink" href="#applications-of-spark-sql" title="Link to this heading">¶</a></h2>
<p>Spark SQL provides a unified data access layer for various data sources, and can even work directly on top of combined data from different sources.</p>
<ul class="simple">
<li><p>ETL: Extract, Transform, Load data from various sources</p></li>
<li><p>Data visualization directly on dataset that exist across multiple data
sources. It skips data movement and processing and directly queries the data
sources.</p></li>
<li><p>Serve as a distributed SQL query engine by providing APIs/SQL queries to
query data across multiple data sources without moving and merging data</p></li>
<li><p>Works as data warehousing and data lake solutions by managing multiple data
sources efficiently and providing a unified interface for data management.</p></li>
</ul>
</section>
<section id="dataframe">
<h2>DataFrame<a class="headerlink" href="#dataframe" title="Link to this heading">¶</a></h2>
<p>The DataFrame API is the high-level abstraction in PySpark, designed to
efficiently handle structured data. Although based on RDDs (Resilient
Distributed Datasets), the DataFrame API provides more additional
functionality, making it more user-friendly for data manipulation of structured
data.</p>
<p>The DataFrame API has several key characteristics. It is distributed, meaning
the data is spread across multiple machines in a cluster for parallel
processing, enhancing scalability and performance. It is also immutable,
meaning once a DataFrame is created, its data cannot be modified, ensuring
stability in data processing workflows. Lastly, it follows a principle of lazy
evaluation, where transformations on the DataFrame are not executed
immediately. Instead, operations are only performed when an action requiring a
result, such as an operation that involves data shuffling, is triggered. This
feature helps optimize the performance of data processing by allowing PySpark
to optimize the execution plan.</p>
<p>To access all functionalities of the DataFrame API, you need to create a
<code class="code docutils literal notranslate"><span class="pre">SparkSession</span></code> object.</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">Create a SparkSession object</span><a class="headerlink" href="#id3" title="Link to this code">¶</a></div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pyspark.sql</span><span class="w"> </span><span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span> \
            <span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[*]&quot;</span><span class="p">)</span><span class="o">.</span> \
            <span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Example&quot;</span><span class="p">)</span><span class="o">.</span> \
            <span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
</div>
<section id="create-dataframes">
<h3>Create DataFrames<a class="headerlink" href="#create-dataframes" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>From RDD</p></li>
</ol>
<blockquote>
<div><div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">Create DataFrames from RDD</span><a class="headerlink" href="#id4" title="Link to this code">¶</a></div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># toDF method</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;John&quot;</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Jane&quot;</span><span class="p">,</span> <span class="mi">35</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Mike&quot;</span><span class="p">,</span> <span class="mi">42</span><span class="p">)]</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">])</span>

<span class="c1"># createDataFrame method</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Alice&quot;</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Bob&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Charlie&quot;</span><span class="p">,</span> <span class="mi">30</span><span class="p">)]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Loading from Data Sources</p></li>
</ol>
<p>Spark SQL supports various data file formats like CSV, JSON, Parquet, ORC,
Avro, etc. It also supports reading data from databases though JDBC or other
adaptors.</p>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">Load DataFrames from data sources</span><a class="headerlink" href="#id5" title="Link to this code">¶</a></div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&quot;path/to/file.csv&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;path/to/file.json&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;path/to/file.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="save-dataframes">
<h3>Save DataFrames<a class="headerlink" href="#save-dataframes" title="Link to this heading">¶</a></h3>
<p>DataFrames can be saved to various data sources like CSV, JSON, Parquet, ORC,
Avro, etc.</p>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">Save DataFrames to data sources</span><a class="headerlink" href="#id6" title="Link to this code">¶</a></div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;csv&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span> \
    <span class="n">option</span><span class="p">(</span><span class="s2">&quot;header&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;path/to/save&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="dataframe-operations-with-functions">
<h3>DataFrame Operations with Functions<a class="headerlink" href="#dataframe-operations-with-functions" title="Link to this heading">¶</a></h3>
<p>The PySpark DataFrame does not directly accept Python functions. Instead,
PySpark provides a rich collection of built-in functions through the
spark.sql.functions package. These built-in functions offer various
capabilities for working with DataFrames, eliminating the need to pass Python
functions directly into DataFrame operations. It’s recommended to learn these
functions as needed, rather than memorizing them all at once, since PySpark
offers a wide variety of tools for data manipulation.</p>
<p>Common built-in functions for DataFrame operations include those for selecting
columns with <code class="docutils literal notranslate"><span class="pre">select()</span></code>, filtering rows with <code class="docutils literal notranslate"><span class="pre">filter()</span></code> or <code class="docutils literal notranslate"><span class="pre">where()</span></code>,
adding new columns with <code class="docutils literal notranslate"><span class="pre">withColumn()</span></code>, and renaming columns using
<code class="docutils literal notranslate"><span class="pre">withColumnRenamed()</span></code>. You can drop unnecessary columns with <code class="docutils literal notranslate"><span class="pre">drop()</span></code> and
sort rows using <code class="docutils literal notranslate"><span class="pre">orderBy()</span></code> or <code class="docutils literal notranslate"><span class="pre">sort()</span></code>. Grouping and aggregating data can
be done through <code class="docutils literal notranslate"><span class="pre">groupBy()</span></code> and <code class="docutils literal notranslate"><span class="pre">agg()</span></code>, while joining DataFrames can be
accomplished using <code class="docutils literal notranslate"><span class="pre">join()</span></code>.</p>
<p>For handling missing data, you can utilize <code class="docutils literal notranslate"><span class="pre">na.drop()</span></code> to remove null values
or <code class="docutils literal notranslate"><span class="pre">na.fill()</span></code> to replace them. Functions like <code class="docutils literal notranslate"><span class="pre">distinct()</span></code> and
<code class="docutils literal notranslate"><span class="pre">dropDuplicates()</span></code> help in finding unique values in the DataFrame. DataFrames
can also be combined using the <code class="docutils literal notranslate"><span class="pre">union()</span></code> function, and reshaped using the
<code class="docutils literal notranslate"><span class="pre">pivot()</span></code> function. Additionally, the <code class="docutils literal notranslate"><span class="pre">explode()</span></code> function can be used to
transform array or map columns into rows.</p>
<p>While built-in functions handle most common operations, PySpark also allows
user-defined functions (UDFs) to handle more complex or custom logic. However,
creating UDFs involves a more complicated mechanism, especially compared to the
use of built-in functions.</p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">PySpark Programming: Spark SQL and DataFrame API</a><ul>
<li><a class="reference internal" href="#outline">Outline</a></li>
<li><a class="reference internal" href="#spark-sql-overview">Spark SQL Overview</a></li>
<li><a class="reference internal" href="#performance-optimizations">Performance optimizations</a></li>
<li><a class="reference internal" href="#applications-of-spark-sql">Applications of Spark SQL</a></li>
<li><a class="reference internal" href="#dataframe">DataFrame</a><ul>
<li><a class="reference internal" href="#create-dataframes">Create DataFrames</a></li>
<li><a class="reference internal" href="#save-dataframes">Save DataFrames</a></li>
<li><a class="reference internal" href="#dataframe-operations-with-functions">DataFrame Operations with Functions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/big_data/pyspark_df.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">IanFang Course Note Hosting  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">PySpark Programming: Spark SQL and DataFrame API</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2021-2024, Ian Fang.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </div>
  </body>
</html>