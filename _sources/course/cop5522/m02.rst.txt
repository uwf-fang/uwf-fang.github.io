****************************************
Module 2: Parallel Hardware and Software
****************************************

Serial System
=============
+ The von Neumann architecture

  * CPU

    * Control unit
    * Arithmetic logic unit (ALU)

  * Memory
  * Input/Output (I/O)

+ Operating system

  * Process

    - Multitasking

  * Thread

+ Cache

  * Definition: A cache is a smaller, faster type of volatile computer memory
    that provides high-speed data access to the processor and stores frequently
    used computer programs, applications, and data.
  * Leveled

    - L1, L2, L3, ... from faster/smaller to slower/larger

  * Cache line
  * Cache address mapping
  * Cache hit/miss
  * Cache eviction policy

+ Virtual memory

  * Page - Fixed length block like a fixed length C array
  * Swapping - unused page moved to secondary storage
  * Virtual address

+ Instruction level parallelism (ILP)

  Internal feature of a CPU to allow parallelism. Hardware dependant.

  * pipeline - unique subunit
  * multiple issue - replicated subunits

+ Thread level parallelism (TLP)

Parallel Hardware
=================
+ Type of parallelism

  * data parallelism
  * task parallelism

+ Categorized with Flynn's taxonomy
+ Single instruction single data (SISD)

  * classic von Neumann

+ Single instruction multiple data (SIMD)

  * Vector processor
  * single core GPUs (Graphical Processing Unit)

+ Multiple instruction multiple data (MIMD)

  * Types

    - shared-memory vs distributed memory
    - homogeneous vs heterogeneous

  * interconnections

    - bus
    - switch

      + direct
      + indirect

    - bisect width/bandwidth

  * cache coherence in shared-memory system

    - solution

      + snooping
      + directory

    - false sharing - different part of a cache line is shared

Parallel Software
=================
+ Focusing on homogeneous MIMD systems
+ Single program multiple data (SPMD) programs

  * shared-memory -> threads
  * distributed-memory -> processes/computation nodes

+ In shared-memory system

  * Communicating by I/O to the shared memory
  * nondeterminism
  * racing condition
  * critical section
  * mutual exclusion lock (MUTEX)
  * thread-safety
  * Libraries

    * POSIX thread
    * OpenMP

+ In distributed-memory system

  * Communicating by message passing
  * The Message-Passing Interface (MPI) standard

    - MPICH
    - OpenMPI

Performance Model
=================
+ Problem size ``N``
+ Speedup ``S``
+ Efficiency ``E``
+ Number of processes/threads/nodes ``P``
+ Amdahl's law
+ Scalability

  * strong

    - plot: ``E/S`` vs ``P`` with fixed ``N``

  * weak

    - plot: ``E`` vs ``P`` with fixed ``N/P`` ratio

+ Timing

Parallel Program Design
=======================
+ Foster's methodology

  * partition
  * agglomeration
  * mapping

Addition Content
================

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   intel-intrinsics
   profiling
