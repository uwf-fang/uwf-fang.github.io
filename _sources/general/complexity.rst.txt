*******************
Complexity Analysis
*******************
Computational complexity analysis plays a pivotal role in the theoretical
analysis of algorithms. It involves assessing how the performance of an
algorithm scales with the size of the input, enabling us to make predictions
about an algorithm's efficiency in terms of time and space usage. This analysis
is crucial for understanding the inherent difficulty of solving computational
problems and for comparing and selecting algorithms for various tasks.

Concepts
========

.. glossary::

  Computational complexity
    The computational resources consumed by an algorithm as a function of the
    size of the problem.

  Time complexity
    The computational time consumed by an algorithm as a function of the size of
    the problem. It is usually described using a function :math:`T(n)`.

  Space complexity
    The computational storage space (usually refer to memory) consumed by an
    algorithm as a function of the size of the problem. **auxiliary space
    complexity** is the space complexity excluding the input data. It is usually
    described using a function :math:`S(n)`.

  Best case, worst case, and average case
    Best case is the scenario in which the algorithm finished with the least
    resource consumed. Worst case is the scenario in which the algorithm
    finished with the most resource consumed. Sometimes we will use average
    case, which is the average over all possible inputs.

  Constant time operation
    Because the accurate measurement of time consumption is impossible, we
    usually count the number of constant time operations to estimate the time
    consumption. A constant time operation is an operation that always consume a
    certain amount of time regardless of the problem size n.

  Growth rate
    How much more resource is needed when the problem size grows. It is usually
    described using simple math functions. It is a simple way to visualize the
    complexity.

  Asymptotic notation
    The math language we use to describe the complexity in a formal way. It
    includes :math:`O` (big-O), :math:`\Omega` (big-omega), :math:`\Theta`
    (big-theta) notations.

  Lower bound
    The best (least resource) complexity of a given algorithm. Related to the best case and the :math:`\Omega` (big-omega) notation.

  Upper bound
    The worst (most resource) complexity of a given algorithm. Related to the worst case and the :math:`O` (big-O) notation.

  Exact bound
    When a same complexity can be found for both the upper and lower bound, it
    is the exact bound :math:`\Theta`

.. image:: /_static/complexity.png
  :width: 400
  :alt: Complexity
  :align: center

Asymptotic notation
===================
A.k.a Bachmannâ€“Landau notation. A mathematical notation system to describe the
limiting behavior of a function.

.. note::

  Time complexity is more useful and is focused in the following sections. Most
  of the rules can be used with space complexity with little modification.

Formal Definitions
------------------
Given :math:`T(n)` the time function of the problem size :math:`n`

We will write :math:`T(n) = O(f(n))` or :math:`T(n) \in O(f(n))` if there
exists a certain positive constant :math:`c` and a problem size :math:`n_{0}`,
such that :math:`T(n) \leq cf(n)` for all :math:`n \geq n_{0}`.

We will write :math:`T(n) = \Omega(f(n))` or :math:`T(n) \in \Omega(f(n))` if
there exists a certain positive constant :math:`c` and a problem size
:math:`n_{0}`, such that :math:`T(n) \geq cf(n)` for all :math:`n \geq n_{0}`.

We will write :math:`T(n) = \Theta(f(n))` if :math:`T(n) = \Omega(f(n))` and
:math:`T(n) = O(f(n))` are both true. Note: the constants are different.

We will write :math:`T(n) = o(f(n))` or :math:`T(n) \in O(f(n))` if there
exists a certain positive constant :math:`c` and a problem size :math:`n_{0}`,
such that :math:`T(n) < cf(n)` for all :math:`n \geq n_{0}`.

.. warning::

  The :math:`=` used in equations like :math:`T(n) = O(f(n))` does not mean
  *equal*! It means *in* or *belong to*.

All notations :math:`O(f(n))`, :math:`\Omega(f(n))`, :math:`T(n) =
\Theta(f(n))` and :math:`T(n) = o(f(n))` define sets of functions that have the
same *order*. Thus, the set will be the same of the function :math:`f(n)` has
the same order. For example, :math:`O(n^2)=O(2*n^2)=O(n^2+3n)=\dots`

Representative Algorithms of Certain Complexity
-----------------------------------------------

.. list-table:: Typical Complexity and Representative Algorithms
   :widths: 20 30 50
   :header-rows: 1

   * - Name
     - Complexity (Big O)
     - Representative Algorithms
   * - Constant
     - :math:`O(1)`
     - Array indexing, Hash table lookups
   * - Logarithmic
     - :math:`O(log(n))`
     - Binary Search, Balanced Binary Search Trees (e.g., AVL, Red-Black)
   * - Linear
     - :math:`O(n)`
     - Linear Search, Most Simple Array/Linked List Operations
   * - Linearithmic
     - :math:`O(n log(n))`
     - Merge Sort, Heap Sort, Quick sort
   * - Quadratic
     - :math:`O(n^2)`
     - Bubble Sort, Selection Sort, Insertion Sort
   * - Polynomial (k > 2)
     - :math:`O(n^k)`
     - Matrix Multiplication (naive approach)
   * - Exponential
     - :math:`O(2^n)`
     - Recursive algorithms without memoization (e.g., naive Fibonacci)
   * - Factorial
     - :math:`O(n!)`
     - Traveling Salesman Problem (naive solution)

Properties
----------
+ :math:`O(f(n))` is a **set** of functions that have the same order as
  :math:`f(n)`.

  + E.g. :math:`O(n^2) = O(2n^2) = O(n^2 + 3n) = \dots` because they all have
    the same order :math:`n^2`.
  + Thus, we will always use the simplest form of :math:`f(n)` to represent the
    set. E.g. :math:`O(2n^2 + 10n - 4) = O(n^2)`. This step is called
    **simplification**.

+ If :math:`T(n) = O(f(n))` and :math:`g(n)` is a function with higher order
  than :math:`f(n)`, then :math:`T(n) = O(g(n))`.

  + E.g. if :math:`T(n) = 3n^2 + 10n + 4 = O(n^2)`, then :math:`T(n) = O(n^3) =
    O(n^4) = \cdots`, All functions with higher order than :math:`n^2` is a
    possible upper bound of :math:`T(n)`.
  + Thus, we should always use the lowest-possible order to represent the set,
    :math:`O(n^2)` in this case. Because higher order functions is always a
    possible upper bound, they become less meaningful.

+ Summary, **We should always use the simplest form of the lowest-possible
  order to represent the set**.

Steps to Find Complexity
========================
+ Option 1: Find :math:`T(n)` and then find :math:`O(f(n))`.
+ Option 2: Estimate :math:`O(f(n))` directly.

Find :math:`T(n)`
-----------------
+ :math:`T(n)` = number of operations
+ Count the number of constant time operations
+ Each statement, and expression evaluation is considered
  to take constant time.

  * E.g. In the pseudo code ``if a < 10 then b = a + 10``, the
    comparison ``a < 10`` and the assignment ``b = a + 10`` are considered
    to take constant time. They both should be counted as 1 operation.

+ Function calls are more complicated. Functions that does not variable
  size input are mostly constant time. Functions that does variable size
  input are more complicate and you better check the reference.

  * ``std::swap, cout <<, getline`` are constant time.
  * ``std::sort, std::search, std::shuffle`` are not constant time.

Find :math:`O` given :math:`T(n)`
---------------------------------
+ Keep the term with highest order. Drop the coefficient and lower order terms
  go provide the simplest form.
+ Example:

  * :math:`T(n) = 20 \times n^2 + 10 \times n + 12 = O(n^2)`
  * :math:`T(n) = 5 \times 2^n + 3 \times n^3 = O(2^n)`

.. list-table:: Common Growth Rates, from low to high order
   :widths: 25 25
   :header-rows: 1

   * - Term
     - Growth Rate
   * - Constant Time
     - :math:`1`
   * - Logarithmic
     - :math:`log(n)`
   * - Linear
     - :math:`n`
   * - Linearithmic
     - :math:`n log(n)`
   * - Quadratic
     - :math:`n^2`
   * - Cubic
     - :math:`n^3`
   * - Exponential
     - :math:`2^n`
   * - Factorial
     - :math:`n!`


Find :math:`O` directly
-----------------------
+ additive (sequential, branch)

  * :math:`O(f(n)) + O(g(n)) = O(f(n) + g(n))`
  * E.g. :math:`O(n^2) + O(n) = O(n^2 + n) = O(n^2)`

+ multiplicative (loop, recursion)

  * for a layer of loop: :math:`O(f(n)) \times O(g(n)) = O(f(n) \times g(n))`
  * complexity of number of iterations :math:`O(f(n))`

    - how problem size is reduced in each iteration`
    - reduce by half each round: :math:`O(log(n))`
    - reduce by a constant each round: :math:`O(n)`

  * complexity in each iteration :math:`O(g(n))`
  * E.g. For merge sort, the number of iterations is :math:`O(log(n))` and the
    complexity in each iteration is :math:`O(n)`. Thus, the total complexity is
    :math:`O(logn) \times O(n) = O(n log(n))`.

Pitfalls
========
+ Confusing :math:`T(n)` to :math:`O(f(n))`
+ Not all function calls are constant time operations (a.k.a. :math:`O(1)`)
